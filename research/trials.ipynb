{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef81bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load and process PDF documents\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_pdf_file(data):\n",
    "    loader = DirectoryLoader(data, glob='*.pdf', loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    return text_splitter.split_documents(extracted_data)\n",
    "\n",
    "# Load and split documents\n",
    "extracted_data = load_pdf_file(data='C:/Users/Osei Tutu Dickson/Desktop/Gen AI/mental-health-chatbot-gen-ai/Data/')\n",
    "text_chunks = text_split(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2718f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of chunks are: 87\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"The number of chunks are:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca709f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Osei Tutu Dickson\\AppData\\Local\\Temp\\ipykernel_13188\\2748704661.py:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set up Pinecone vector store\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Pinecone setup\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"mentalbot\"\n",
    "\n",
    "# Create index if not exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "# Store documents in Pinecone\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize RAG pipeline\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LLM setup\n",
    "HUGGINGFACE_API_TOKEN = os.environ.get(\"AI_API_KEY\")\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"flan-t5-small\",\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_TOKEN,\n",
    "    temperature=0.3,\n",
    "    max_new_tokens=256,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Prompt template\n",
    "system_prompt = \"\"\"You are a knowledgeable AI assistant for mental health topics. \n",
    "When answering about sensitive subjects:\n",
    "1. Use ONLY the provided context\n",
    "2. Be factual but compassionate\n",
    "3. If uncertain, say \"I'm not certain but here's what I know...\"\n",
    "4. Never make up information\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Initialize retriever from existing Pinecone index\n",
    "retriever = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ").as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Create chains\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa569152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during query execution: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Osei Tutu Dickson\\AppData\\Local\\Temp\\ipykernel_13188\\1494576166.py\", line 18, in <module>\n",
      "    response1 = rag_chain.invoke({\"input\": query1})\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5431, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py\", line 511, in invoke\n",
      "    return self._call_with_config(self._invoke, input, config, **kwargs)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1940, in _call_with_config\n",
      "    context.run(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\config.py\", line 428, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py\", line 497, in _invoke\n",
      "    **self.mapper.invoke(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3774, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3774, in <dictcomp>\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\concurrent\\futures\\_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3758, in _invoke_step\n",
      "    return context.run(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5431, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3047, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 389, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 766, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 973, in generate\n",
      "    return self._generate_helper(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 792, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1547, in _generate\n",
      "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py\", line 312, in _call\n",
      "    response_text = self.client.text_generation(\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 2296, in text_generation\n",
      "    provider_helper = get_provider_helper(self.provider, task=\"text-generation\", model=model_id)\n",
      "  File \"c:\\Users\\Osei Tutu Dickson\\miniconda3\\envs\\menbot\\lib\\site-packages\\huggingface_hub\\inference\\_providers\\__init__.py\", line 191, in get_provider_helper\n",
      "    provider = next(iter(provider_mapping)).provider\n",
      "StopIteration\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Query the RAG model\n",
    "\n",
    "# Example questions based on your generated mental health book content\n",
    "query1 = \"What is depression and what are its common symptoms?\"\n",
    "query2 = \"Explain the difference between obsessions and compulsions in OCD.\"\n",
    "query3 = \"What does the book say about coping with anxiety?\"\n",
    "query4 = \"When should someone seek professional help for mental health issues?\"\n",
    "query5 = \"What are the common types of eating disorders?\"\n",
    "query6 = \"What are positive symptoms of schizophrenia?\"\n",
    "query7 = \"How do I deal with chronic stress?\"\n",
    "query8 = \"Can you describe the main characteristics of Bipolar Disorder?\"\n",
    "query9 = \"What defines a Substance Use Disorder and what are its signs?\"\n",
    "\n",
    "# Use the invoke method to get a response\n",
    "# Ensure rag_chain is not None before attempting to invoke\n",
    "if rag_chain is not None:\n",
    "    try:\n",
    "        response1 = rag_chain.invoke({\"input\": query1})\n",
    "        print(f\"Question: {query1}\\nAnswer: {response1['answer']}\\n\")\n",
    "\n",
    "        response2 = rag_chain.invoke({\"input\": query2})\n",
    "        print(f\"Question: {query2}\\nAnswer: {response2['answer']}\\n\")\n",
    "\n",
    "        response3 = rag_chain.invoke({\"input\": query3})\n",
    "        print(f\"Question: {query3}\\nAnswer: {response3['answer']}\\n\")\n",
    "\n",
    "        response4 = rag_chain.invoke({\"input\": query4})\n",
    "        print(f\"Question: {query4}\\nAnswer: {response4['answer']}\\n\")\n",
    "\n",
    "        response5 = rag_chain.invoke({\"input\": query5})\n",
    "        print(f\"Question: {query5}\\nAnswer: {response5['answer']}\\n\")\n",
    "\n",
    "        response6 = rag_chain.invoke({\"input\": query6})\n",
    "        print(f\"Question: {query6}\\nAnswer: {response6['answer']}\\n\")\n",
    "\n",
    "        response7 = rag_chain.invoke({\"input\": query7})\n",
    "        print(f\"Question: {query7}\\nAnswer: {response7['answer']}\\n\")\n",
    "\n",
    "        response8 = rag_chain.invoke({\"input\": query8})\n",
    "        print(f\"Question: {query8}\\nAnswer: {response8['answer']}\\n\")\n",
    "\n",
    "        response9 = rag_chain.invoke({\"input\": query9})\n",
    "        print(f\"Question: {query9}\\nAnswer: {response9['answer']}\\n\")\n",
    "\n",
    "        # You can also get the source documents that were used by the retriever\n",
    "        # print(\"Source Documents for query1:\", response1['context'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during query execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"RAG chain is not initialized. Please ensure Cell 3 ran successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
